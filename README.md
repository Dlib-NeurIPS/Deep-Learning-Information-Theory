
# Deep Learning and Information Bottleneck Principle
This repository contains the code of the paper "A Probabilistic Representation for Deep Learning:Delving into The Information Bottleneck Principle" submitted to NeurIPS 2021. 

## Prerequisites
Python 3.7

Tensorflow 1.15

Keras 2.2.4

Numpy

Matplotlib

Please note that the code is senstive to the version of the above packages. To make sure the code can be successfully reimpelmented, we recommond to create an anaconda virtual environment, in which install all the packages with the aforementioned versions.

All the codes are placed in the three folders: (i) Simulations on the bechmark dataset, (ii) Comparison to non-parametric models, and (iii) Simulations on the bechmark dataset.

## Simulations on the bechmark dataset
### test_MLP_Gibbs_IT.py 
It generates the Figure 3, Figure 4 (Right), and Figure 5.


2. Showing the distribution of the first fully connected hidden layer as Gibbs distribution.

![mlp_f1_gibbs](Simulations/Img_MLP_F1_Gibbs.png)

## Comparison to non-parametric models

## Simulations on the bechmark dataset, namely MNIST and Fashion-MNIST

